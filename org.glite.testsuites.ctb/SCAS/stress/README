##############################################################################
#
# AUTHORS: Gianni Pucciani (CERN), Andrey Kiryanov (PNPI)
# CREATED: 8 December 2008
#
##############################################################################

The files in this directory have been created and used to stress test
the first releases of the SCAS service in the multi-user pilot
jobs scenario where GLEXEC is installed on the WNs.


Files
-----

- setup_test.sh -
Usage: ./setup_test.sh

This script is the one used to setup the test. It uses a configuration file 
(setup_test.cfg) explained later.
In order to run the script, you first have to launch the ssh-agent:
#eval `ssh-agent`
#ssh-add /home/<user>/.ssh/id_rsa
#ssh-add -l

The script is called without arguments, the machine names used to launch glexec requests
are stored in the setup_test.cfg file.
For each defined machine the script logs into it and:
- gets an AFS token with a script stored in $get_afstoken_script (from the cfg file)
- touch a log file with the name of the machine in $log_location (from the cfg file)
to store the results of the glexec calls
- touch a log file in the root directory to store log information printed by the setup
node script (see below)
- calls the setup node script stored in $setup_node_script (from the cfg file) 
passing as arguments an index used to discriminate the node plus the log file (first).

- setup_test.cfg -
Configuration file for the previous script.
It exports the following variables:
setup_node_script
log_location
get_afstoken_script
get_afstokenlocal_script
pass_file
afs_pass_file
afs_user
users_certs_dir
glexec_stress_script
proxy_ren_script
<hostnames> of the WNs

- setup_node.sh -
Usage: ./setup_node.sh <index> <log file>
    <index> integer number to identify the node.
    <log file> the file that will be used to log data and start the test.

This script is called by the previous one.
It is used to setup a node on which the test will run and call the real test
script $glexec_stress_script.
The operations done are:
 *retrieve and start the proxy renewal daemon $proxy_ren_script to get (periodicity is hardcoded) proxy and AFS token
 *retrieve and launch the glexec test
 *when the test is finished kill the proxy renewal daemon.

- glexec_stress_test.sh -
Usage: ./glexec_stress_test.sh -f <log file> {-n <number>|-d <end date>} -i <index>
      <number> an integer number to state the number of tests to be done.
      <end date> a date until when to execute the test.
                 The format must be YYYYMMDDhhmm (returned by date +%Y%m%d%H%M).
                 E.g. 20 January 2009 at 8am => 200901200800
      <index> an integer number to identify the node

This script is called by the previous one.
It is the one used on the WN to launch a certain amount of glexec commands.
In particular, the script will execute the glexec test either n times or until
a specified date. The starting of test is triggered when the word 'START' is 
found in the specified log file.
The index passed with the -i option is used to distinguish node and test user.
The data file named <log file>_data will be created in the same directory as the
<log file> and will be used to store pairs (timestamp , response time of glexec call).
In the <log file> the output of the glexec call is stored.
Two more lines are stored at the end of the log file, the number of glexec
executions and the number of errors (glexec returned non zero code).
Another file <log file>_error is created to record the success or failure 
of each call, with the format (timestamp, {0|1}), where 0 means success and
1 means error. 

- glexec_stress_test2.sh -
This script is an enanched version of the previous one, to be used with multiple
user credential on each host, following the predefined schema (10 users on each host).
The usage and workflow is the same as glexec_stress_test.sh.



- verify_glexec_output.sh -
Usage: ./verify_glexec_output.sh <mapped_user> <output file>
    <mapped_user> Unix user that should be returned by the glexec test
                  In case of pool accounts one can specify the prefix (without numbers).
    <output file> The file containing the output of the glexec test

This script is used to verify the output produced by the glexec_stress_test file,
which should contain just the word 'START' (inserted by the user to trigger the start
of the stress test) and the mapped user returned by the command:
$GLITE_LOCATION/sbin/glexec "/usr/bin/whoami", plus the last two comment lines.
Since the script  uses the regular expression [^$user], in case of pool accounts
one can just specify the prefix (E.g 'pildtm' can be specified to verify accounts
like 'pildtm01', 'pildtm10' etc..)

The file to be provided as second argument is the one created by the script
setup_test.sh in $log_location (from cfg file), and it is named as the host that will 
write on that file.

- scas-mon.pl -
Usage: ./scas-mon.pl

This is a monitor script for SCAS daemon. Upon startup it will look through process
table for an active SCAS daemons.
A log file /var/tmp/scas-mon.log will be written in CSV format with the following
fields: Timestamp in seconds since epoch, Load1, Load5, Load15, Number of open FDs,
Memory consumption in kilobytes and number of SCAS processes.
Records are made every 5 seconds. Script does not accept any parameters and
all reconfiguration (process name, log file path, time interval) must be done
by editing the script (but default parameters are just fine).
Script will stay in memory even if all SCAS daemons die. It will pick ip new ones
as soon as they are started.

- combine.pl -
Usage: ./combine.pl <log1> <log2> ...

This script is used to combine log files from other scripts into one. Input files
should be specified as a command line arguments and output is written to stdout
(you probably want to redirect it somewhere). Input files should be in CSV format
with timestamp as a first field of every record. Script will add fields from specified
files in the order of appearance. All gaps will be filled automatically, values with
the same timestamp will be replaced by a mean value.
This script is useful for importing log information into applications like OpenOffice
or Gnuplot for further analysis.

- mkugconf.pl -
Usage: ./mkugconf.pl -v <vo>,... [-g <group>,...] [-r <role>,...] [-n <num>]
	<vo>	- VO name
	<group> - VOMS group
	<role>	- VOMS role
	<num>	- number of pool accounts per combination (10 by default)

This script is intended for easy creation of users.conf and groups.conf files for
YAIM. It accepts a comma-separated lists of VOs, groups and roles and creates both
files with all possible combinations.

- userwipe.pl -
Usage: ./userwipe.pl <uid regexp>

This script will erase all (pool) accounts with UIDs that match the regexp.
Corresponding home directories and groups will also be removed. You should
type "yes" on a terminal in order to let the script do its dirty job.

- proxy_renewal.sh -
Usage: ./proxy_renewal.sh <index>"
  <index> integer number to discriminate clients in the test"

This script creates a proxy certificate for test users 50x and
renews the proxy every 4 hours (hardcoded).
The index that must be given as argument is used to choose the user.
The proxy is created into the current directory and copied
into the pilot user account: dteamdteampilotx.
With the same periodicity the AFS token for the account specified
in the configuration file is acquired.
This scripts is called by setup_node.sh and the output is redirected
to proxy_renewal.out in the root home directory.

- proxy_renewal2.sh -
Usage: ./proxy_renewal.sh <index>"
  <index> integer number to discriminate clients in the test"

This script is an enhanced version of the previous one.
It creates a proxy certificate for 10 (hardcoded) test users 
from 5<index>0 to 5<index>9 and renews the proxy every 4 hours (hardcoded).
The proxy is created into the current directory and copied
into the pilot user account: dteamdteampilot<index> as x509up_u501_<0:9>.
With the same periodicity the AFS token for the account specified
in the configuration file is acquired.
This scripts is called by setup_node.sh and the output is redirected
to proxy_renewal.out in the root home directory.

- start_stress_test.sh -
Usage: ./start_stress_test.sh

This file can be used to start the stress test on all the machines
at the same time. It will insert the string 'START' in the hosts
log files to trigger the start of the stress test.

- start_stress_test_grad.sh -
Usage: ./start_stress_test_grad.sh <hours>
  <hours> number of hours between nodes activation

This file can be used to start the stress specifying an interval
in hours between two activation. Giving '0' as argument it behaves
as the previous script.
This script also starts in background the getHourlyErrorRate.sh script
to collect error rate data in $log_location/hourlyErrorRate.txt (every 1h hardcoded).

- mkRespTimePlots.sh -
Usage ./mkRespTimePlots [<log_location>] [<datafile>]
   <log_location> a directory were *_data_norm files are stored
   <datafile> a file with time,resptime data

This script is used to create graphs (gnulot .plt and ps files) of the response time
of the glexec calls. The input files are the *_data files
generated by glexec_stress_test.sh script.
A single datafile or a directory where datafiles are stored can be
given as arguments.

- filterRespTimeData.sh -
Usage ./filterRespTimeData.sh <input file>
  <input file> a file *_data with response time data

It prints the number of requests whos response time is included
in some pre-defined time intervals.

- filterRespTimePlot.sh -
Usage ./filterRespTimePlot.sh <input file>
  <input file> a file *_data with response time data

This script filters the response time data files and produce an
output file named filt_<datafile> with values to create a frequency
histogram. The values used in the filtering are hardcoed: from 1
to 20 seconds with a step of 0.1.
This file must be given in input to the mkHistoRespTimePlot.sh plots.

- mkHistoRespTimePlot.sh -

Usage ./mkRespTimePlots  <datafile>
   <datafile> a file with (interval end,frequency) data

It generates a frequency histogram plot in histo_<datafile_noext>.png
using the data file produced by filterRespTimePlot.sh.

- mkMemConsPlot.sh -
Same as above but print the memory consuption plot, to be used with
the scas monitoring log file.

- getErrorRate.sh -
Usage: getErrorRate <logs_dir>
  <logs_dir> log files directory

This is a simple script to get the error rate (%) from the log files.
It can be used to retrieve temporary information while the test
is running.
The output is like the following:
-------------------------------
Sun Feb  8 10:02:14 CET 2009
-------------------------------
Total requests: 1658331
Total errors: 616
Error rate: .03714578090863645400
-------------------------------


- getHourlyErrorRate.sh -
Usage: ./getHourlyErrorRate.sh <logs_dir> <hours>"
  <logs_dir> log files directory
  <hours> time interval in hours

This script enter an infinite cycle where it prints,
every <hous>, the pair timestamp,errorRate to the
file hourlyErrorRate (in $logs_dir).
The timestamp is in seconds from the Unix epoch and the
error rate is the total error rate (all the nodes) in %a.
A file freqency.txt will be created in the same directory
with requests per seconds computed every hour.

- checkForErrors.sh -

Usage: checkForErrors.sh <logdir>
 <logdir> the directory where the log files are stored

This file produces the correct results only when using 
during the test, not at the end of the test.
It assumes that:
 - 10 machines are used
 - 10 users on each machine are used
 - all the WNs are active

It prints the number of errors found.

- mkErrorDistrPlots.sh -

Usage ./mkErrorDistrPlots.sh  [<log_location>] [<errorfile>]"
   <log_location> a directory were *_data_norm files are stored"
   <errorfile> a file with time,{0|1} pairs"

It prints the distribution of the errors over the time of the test.
Each point at 1 on the Y axis is an error
It prints the distribution of the errors over the time of the test.
Each point at 1 on the Y axis is an error..

- mkHourlyErrorsPlot.sh -
Usage ./mkHourlyErrorsPlot.sh <datafile>
   <datafile> the hourly errors data file


It creates a plot for the hourly errors.

- mkHourlyFrequencyPlot.sh -
Usage ./mkHourlyFrequencyPlot.sh <datafile>
   <datafile> the hourly frequency data file

It creates a plot for the hourly frequencies (requests per seconds).


- normalize_time.sh -
Usage $0 <file> <t0>
  <file> the data file to normalize
  <t0> start time

This script is used to normalize the time; t0 is subtracted to all the
timestamps.

- normalize_all.sh -
Usage $0 <t0> <log_location>
  <t0> start time
  <log_location> directory where all data files are stored

This file is used to normalize the time in all the data files (*_data, *_error and scas*.log).
It uses the previous script to do the job.


- setup_test.exp -

This Expect script is used to automatically login on several machines
and call a script to setup the test on each machine.
The usage is the following:
expect setup_test.exp <passwords file>  <node setup script> <log file location>

The passwords file is a text file with the list of machines with the 
associated root password separated by a colon ':'.
The first line is the user AFS login/pass.
E.g.
----------------------------------
afs_user:afs_pass
machine1:pass1
machine2:pass2
...
machinen:passn
----------------------------------

No comments can be included in this file.
The node setup script is a script that will be copied under /root of the
target machine and executed. This script is supposed to prepare the 
environment for the test on the target machine.
If the execution of the setup node is successful (i.e the node setup script 
returns '0'), an empty file with the name of the target machine will be 
created under the location specified by the last argument, the log file 
location.

It is important to remind that the node setup script must be available on
an AFS space where it can be retrieved.
The log file location must also be on an AFS space.

KNOWN ISSUES
From time to time the script signals an error in the log file
creation step, or it does not signal errors but the file does not
show up.
This problem appears randomly and it is usually solved calling the
script after a short pause of 10-30 seconds. 
Due to this problem this file has not been used.
In case an automatic ssh login can be done with the ssh-agent,
the next script must be used.


________________________________________________________________

The procedure to follow for the WN installation is described in
wn_installation.txt.

- vomses.tar.gz -
This file is used to create the /opt/glite/etc/vomses directory
that is not present in a WN installation but is used by the
proxy_renewal script.
This file must be extracted as explained in wn_installation.txt.

----------------------------------------------------------------
At the end of a test, to create all the necessary graphs:

1) create a new directory to store working files and save original results
in a zipped archive.

mkdir /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod
tar -cvzf 090306_4days.tar.gz lxb760*.*
mv lxb760*.* /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod

2) normalize time values to make time axis starting at 0 sec 
./normalize_all 1236343721 /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod

3) create response time graphs 
./mkRespTimePlots.sh /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod

4) normalize scas-mon.log file
./normalize_time.sh /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod/scas-mon.log 1236343721 

5) create scas service memory consumption plot and move the file to the mod directory
./mkMemConsPlots.sh /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod/scas-mon.log_norm
mv scas-mon.ps scas-mon.plt scas-mon.log_norm /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod

6) look at the graph and enjoy!

7) get number of calls and number of errors
./getErrorRate.sh /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod
-------------------------------
Tue Mar 10 11:14:24 CET 2009
-------------------------------
Total requests: 3306508
Total errors: 1
Error rate: .00003024338667863400
-------------------------------

8) to create a frequency histogram of the response time graph in with Y axis in logaritmic scale:
./filterRespTimePlot.sh /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod/lxb7606v1.cern.ch_data_norm
(it takes some time)
./mkHistoRespTimePlot.sh /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod/filt_lxb7606v1.cern.ch_data_norm 
mv histo_filt_lxb7606v1.png filt_lxb7606v1.plt filt_lxb7606v1.cern.ch_data_norm /afs/.cern.ch/project/gd/sa3_test_result/090306_4days/mod

